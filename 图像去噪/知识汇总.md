## CNN-based


## Transformer-based
### SwinIR
- [x] 为什么ViT不适合作为backbone，但Swin Transformer适合？
- [ ] 应用在low-level和high-level怎么迁移？

#### 先补一下swin transformer
window：序列缩短 + shift：交互 
**可以提取多尺度特征**
1. 分层的感受野不同，可以解决尺度问题
2. unet的skip connection  
----------------------------------------
- window： 
实际上利用了locality的先验知识，即语义相似的部分大概率是相连的  

- patch merging：  
类似于卷积中的pooling操作，将小patch合成大patch，可以提取多尺度特征
类似于pixel shuffle的反过程？

- **shift window**:
实际上是一个overlap的，能够让上下文更好理解的

![image](https://github.com/user-attachments/assets/a4856c11-d481-4c40-b9d0-388f2292ec52)

mask 循环移位
![image](https://github.com/user-attachments/assets/4c963b34-82e6-4bcc-9295-f11feda941b5)
通过循环移位来维持仍然只计算4个窗口且每个窗口中的patch数量一致，mask来保证每个窗口之中不相连的patch不计算注意力，最后将它们还原
掩码可视化：
![image](https://github.com/user-attachments/assets/b2644a47-0881-49aa-9786-8193e07b58c7)
![image](https://github.com/user-attachments/assets/9b17e423-f91c-44ac-8645-225c4de83aa9)
