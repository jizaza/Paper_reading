## torch.distributed
```python
torch.distributed.init_process_group(
    backend='nccl',
    init_method=None,
    timeout=datetime.timedelta(seconds=1800),
    world_size=-1,
    rank=-1,
    store=None,
    group_name='default',
    **kwargs
)
```
参数说明：
- backend：指定分布式后端的名称，例如 ‘nccl’、‘gloo’ 或 ‘mpi’。
- init_method：初始化方法的 URL 或文件路径。默认为 None，表示使用默认的初始化方法。
- timeout：初始化过程的超时时间，默认为 1800 秒。
- world_size：参与分布式训练的总进程数。默认为 -1，表示从环境变量中自动获取。
- rank：当前进程的排名。默认为 -1，表示从环境变量中自动获取。
- store：用于存储进程组信息的存储对象。默认为 None，表示使用默认存储。
- group_name：进程组的名称，默认为 ‘default’。
- **kwargs：其他可选参数，根据不同的分布式后端而定。
